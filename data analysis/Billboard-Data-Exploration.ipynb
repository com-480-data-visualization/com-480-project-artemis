{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/walkerkq/musiclyrics\n",
    "\n",
    "Billboard has published a Year-End Hot 100 every December since 1958. The chart measures the performance of singles in the U.S. throughout the year. Using R, I’ve combined the lyrics from 50 years of Billboard Year-End Hot 100 (1965-2015) into one dataset for analysis. You can download that dataset here.\n",
    "\n",
    "The songs used for analysis were scraped from Wikipedia’s entry for each Billboard Year-End Hot 100 Songs (e.g., 2014). This is the year-end chart, not weekly rankings. Many artists have made the weekly chart but not the final year end chart. The final chart is calculated using an inverse point system based on the weekly Billboard charts (100 points for a week at number one, 1 point for a week at number 100, etc).\n",
    "\n",
    "I used the xml and RCurl packages to scrape song and artist names from each Wikipedia entry. I then used that list to scrape lyrics from sites that had predictable URL strings (for example, metrolyrics.com uses metrolyrics.com/SONG-NAME-lyrics-ARTIST-NAME.html). If the first site scrape failed, I moved onto the second, and so on. About 78.9% of the lyrics were scraped from metrolyics.com, 15.7% from songlyrics.com, 1.8% from lyricsmode.com. About 3.6% (187/5100) were unavailable.\n",
    "\n",
    "The dataset features 5100 observations with the features rank (1-100), song, artist, year, lyrics, and source. The artist feature is fairly standardized thanks to Wikipedia, but there is still quite a bit of noise when it comes to artist collaborations (Justin Timberlake featuring Timbaland, for example). If there were any errors in the lyrics that were scraped, such as spelling errors or derivatives like \"nite\" instead of \"night,\" they haven't been corrected.\n",
    "\n",
    "Full analysis can be found here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lyrics Top-100 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_incomplete = \"datasets/billboard_lyrics_1964-2015.csv\"\n",
    "data_file = \"datasets/billboard_full.csv\"\n",
    "\n",
    "df_incomplete = pd.read_csv(data_file_incomplete, encoding = \"ANSI\") # utf-8 encoding doesn't work somehow :(\n",
    "df = pd.read_csv(data_file, index_col=0, header=0, sep=\",\") \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.Artist.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of years in Top 100 per Song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count = df.groupby([\"Artist\", \"Song\"]).Year.agg(list).to_frame()\n",
    "df_count[\"Count\"] = df_count.Year.apply(len)\n",
    "df_count = df_count.sort_values(\"Count\", ascending = False)\n",
    "df_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(df_count[\"Count\"].value_counts(), labels = [1, 2], autopct='%1.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_genre = df.Genre.value_counts()\n",
    "vc_genre = vc_genre[vc_genre > 70] # Filter very unfrequent\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.pie(vc_genre.values, labels = vc_genre.index, autopct='%1.1f%%')\n",
    "# plt.savefig(\"images/genre_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of songs in top 100 per artist (if a song is twice, is counted twice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.Artist.value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songCounts = df.groupby(\"Artist\").Song.count()\n",
    "df_rndArtist = df_songCounts.to_frame().reset_index().groupby(\"Song\").agg(list)\n",
    "df_rndArtist[\"Artist\"] = df_rndArtist[\"Artist\"].apply(lambda a : np.random.choice(a, 1)[0])\n",
    "\n",
    "df_labels = pd.DataFrame(range(1, df_songCounts.max() + 1), columns = [\"Song\"])\n",
    "df_labels[\"Artist\"] = \"\"\n",
    "df_labels = df_labels.set_index(\"Song\")\n",
    "df_labels.update(df_rndArtist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df_songCounts.value_counts()\n",
    "tmp[tmp.index <= 3].sum() / tmp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bins = range(1, df_songCounts.max() + 1)\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.hist(df_songCounts, bins = bins)\n",
    "plt.xticks(bins, df_labels[\"Artist\"], rotation='vertical')\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Number of times appearing in Top-100 per Artist, with randomly selected artist per bin\")\n",
    "# plt.savefig(\"images/songs_per_artist.png\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lyrics statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lyrics_lengths = df.Lyrics.apply(lambda s : len(s.split(\" \")))\n",
    "\n",
    "bins = range(1, 1000)\n",
    "plt.hist(lyrics_lengths, bins = bins)\n",
    "plt.title(\"Number of words per song\")\n",
    "plt.savefig(\"words.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_lengths_unique = df.Lyrics.apply(lambda s : len(set(s.split(\" \"))))\n",
    "\n",
    "bins = range(1, 1000)\n",
    "plt.hist(lyrics_lengths_unique, bins = bins)\n",
    "plt.title(\"Number of unique words per song\")\n",
    "plt.savefig(\"unique_words.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lyrics_lengths_unique.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusterisation of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag, sent_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def lemmatize(token, pos_tag):\n",
    "    tag = {\n",
    "        'N': wn.NOUN,\n",
    "        'V': wn.VERB,\n",
    "        'R': wn.ADV,\n",
    "        'J': wn.ADJ\n",
    "    }.get(pos_tag[0], wn.NOUN)\n",
    "    return lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "def preprocess_lyrics(lyrics):\n",
    "    tagged_tokens = pos_tag(wordpunct_tokenize(lyrics))\n",
    "    preprocessed = [lemmatize(token, tag) for (token, tag) in tagged_tokens if not token in stop_words]\n",
    "    return \" \".join(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_lyrics = df.Lyrics.fillna(\"\").apply(preprocess_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the count vectorizer with the English stop words\n",
    "count_vectorizer = CountVectorizer(stop_words='english', preprocessor = None)\n",
    "# Fit and transform the processed titles\n",
    "count_data = count_vectorizer.fit_transform(preprocessed_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "# Tweak the two parameters below\n",
    "number_topics = 5\n",
    "number_words = 4\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components=number_topics, n_jobs=-1)\n",
    "lda.fit(count_data)\n",
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, count_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_10 = lda.transform(count_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(vectors_10[:100, 0], vectors_10[:100, 1], color = \"red\")\n",
    "plt.scatter(vectors_10[5000:, 0], vectors_10[5000:, 1], color = \"blue\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(2)\n",
    "vectors_2 = svd.fit_transform(vectors_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(vectors_2[:100, 0], vectors_2[:100, 1], color = \"red\")\n",
    "plt.scatter(vectors_2[5000:, 0], vectors_2[5000:, 1], color = \"blue\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "documents = [TaggedDocument(doc.split(\" \"), [i]) for i, doc in enumerate(df.Lyrics.fillna(\"\"))]\n",
    "model = Doc2Vec(documents, vector_size=300, window=4, min_count=1, workers=4, epochs = 10, dbow_words = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I cant get no satisfaction\"\n",
    "vector = model.infer_vector(sentence.split(\" \"))\n",
    "documents[model.docvecs.most_similar([vector])[1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[4480]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(2)\n",
    "vectors_2 = svd.fit_transform(model.docvecs.vectors_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(vectors_2[:100, 0], vectors_2[:100, 1], color = \"red\")\n",
    "plt.scatter(vectors_2[5000:, 0], vectors_2[5000:, 1], color = \"blue\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "tqdm_notebook().pandas()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_filepath = \"datasets/events_full.csv\"\n",
    "\n",
    "df_events = pd.read_csv(event_filepath, index_col=0)\n",
    "df_events.xs(0).Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_extractor(cols, song = False):\n",
    "    \"\"\"\n",
    "    Parametrable extraction\n",
    "    \"\"\"\n",
    "    ignore_entities = [\"CARDINAL\", \"MONEY\", \"ORDINAL\", \"QUANTITY\", \"TIME\"]\n",
    "    def extract_entities(row):\n",
    "        \"\"\"\n",
    "        Actual extraction\n",
    "        \"\"\"\n",
    "        entities = []\n",
    "        for col in cols:\n",
    "            if type(row[col]) == str:\n",
    "                entities += [(ent.text, ent.label_) for ent in nlp(row[col]).ents if ent.label_ not in ignore_entities]\n",
    "                \n",
    "        if song : \n",
    "            entities.append((row[\"Artist\"], \"PERSON\"))\n",
    "            entities.append((row[\"Song\"], \"WORK_OF_ART\"))\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    return extract_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_cols = [\"Content\", \"Summary\"]\n",
    "df_events[\"Entities\"] = df_events.progress_apply(entity_extractor(extraction_cols), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Entities\"] = df.progress_apply(entity_extractor([\"Lyrics\"]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_refs(song_rows):\n",
    "    refs = []\n",
    "    for entity, label in song_rows.Entities:\n",
    "        for i, row in df_events.Entities.iteritems():\n",
    "            ents_lower = [ent.lower() for ent, lab in row]\n",
    "            if any([entity in low_ent or low_ent in entity for low_ent in ents_lower]):\n",
    "                refs.append(i)\n",
    "    return refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add references to events in songs\n",
    "\"\"\"\n",
    "df[\"Refs\"] = df.progress_apply(find_refs, axis = 1)\n",
    "df[\"Refs\"] = df[\"Refs\"].apply(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filteredRefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SimonRoquette\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aff0d2a7ab8424e82e7a1405daebf2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Add references to songs in events\n",
    "\"\"\"\n",
    "df_events[\"filteredRefs\"] = [[] for i in range(len(df_events))]\n",
    "for i_song, refs in tqdm_notebook(df[\"filteredRefs\"].iteritems()):\n",
    "    for i_event in refs:\n",
    "        df_events.iloc[i_event][\"filteredRefs\"].append(i_song)\n",
    "        \n",
    "df_events[\"filteredRefs\"] = df_events[\"filteredRefs\"].apply(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add references to songs in events\n",
    "\"\"\"\n",
    "df_events[\"Refs\"] = [[] for i in range(len(df_events))]\n",
    "for i_song, refs in tqdm_notebook(df[\"Refs\"].iteritems()):\n",
    "    for i_event in refs:\n",
    "        df_events.iloc[i_event][\"Refs\"].append(i_song)\n",
    "        \n",
    "df_events[\"Refs\"] = df_events[\"Refs\"].apply(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Refs\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.xs(4971).Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = dict()\n",
    "for ents in df.Entities.apply(lambda x : [e[0] for e in x]).values:\n",
    "    for ent in ents :\n",
    "        if ent in s:\n",
    "            s[ent] +=1\n",
    "        else :\n",
    "            s[ent] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = {k: v for k, v in sorted(s.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events[df_events.Wikipedia.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events[(df_events.Wikipedia.isna()) & df_events.Content.str.contains(\"Anniversary\")].Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events[(df_events.Year == 2001)& (df_events.Month == \"September\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events.xs(811).Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_events.xs(811).Refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df [(df.Year > 2001) & (df.Lyrics.str.contains(\"twin\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "See the all the types of entities recognized\n",
    "\"\"\"\n",
    "\n",
    "ent_types = set()\n",
    "for s in df[\"Entities\"].apply(lambda x : set([e[1] for e in x])):\n",
    "    for ent in s:\n",
    "        ent_types.add(ent)\n",
    "ent_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"songs_with_refs.csv\")\n",
    "df_events.to_csv(\"events_with_refs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/songs_with_refs.pickle\", \"wb\") as f :\n",
    "    pickle.dump(df, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(\"datasets/events_with_refs.pickle\", \"wb\") as f :\n",
    "    pickle.dump(df_events, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/songs_with_refs.pickle\", \"rb\") as f :\n",
    "    df = pickle.load(f)\n",
    "    \n",
    "with open(\"datasets/events_with_refs.pickle\", \"rb\") as f :\n",
    "    df_events = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "all_docs = np.concatenate((df.Lyrics.fillna(\"\"), df_events.Content.fillna(\"\")))\n",
    "\n",
    "documents = [TaggedDocument(doc.split(\" \"), [i]) for i, doc in enumerate(all_docs)]\n",
    "model = Doc2Vec(documents, vector_size=300, window=4, min_count=1, workers=4, epochs = 10, dbow_words = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics2events_sim = []\n",
    "for i in tqdm_notebook(range(len(df))):\n",
    "    sims = model.docvecs.most_similar(positive = [model.docvecs[i]], topn=100, clip_start=len(df))[1:]\n",
    "    sims = [x for x in sims if (x[0] - len(df)) in df.xs(i).Refs]\n",
    "    sims_refs = sorted(sims, key = lambda x : x[1], reverse=True)[:10]\n",
    "    lyrics2events_sim.append(sims_refs)\n",
    "df[\"Similar\"] = lyrics2events_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df.Similar.apply(len) > 0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SimonRoquette\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\SimonRoquette\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\SimonRoquette\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\SimonRoquette\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\SimonRoquette\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\SimonRoquette\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AlbertModel, AlbertTokenizer\n",
    "from tqdm import tqdm_notebook\n",
    "import torch\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-large-v2')\n",
    "model = AlbertModel.from_pretrained('albert-large-v2')\n",
    "input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(input_ids)\n",
    "last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\"\n",
    "    model.cuda()\n",
    "else:  \n",
    "    dev = \"cpu\"  \n",
    "device = torch.device(dev)  \n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"word_count\"] = df[\"Lyrics\"].apply(lambda x : len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_tokenized = torch.Tensor(tokenizer.batch_encode_plus(df.Lyrics,\n",
    "                                                            max_length =512,\n",
    "                                                           pad_to_max_length=True,\n",
    "                                                           padding_side = \"right\",\n",
    "                                                           add_special_tokens=True)[\"input_ids\"]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SimonRoquette\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d6be9239cf4a8bb9b43abee362f7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=640.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lyrics_vectorized = []\n",
    "batch_size = 8\n",
    "with torch.no_grad():\n",
    "    for i in tqdm_notebook(range(0, len(df), batch_size)):\n",
    "        batch = lyrics_tokenized[i: min(i + batch_size, len(df))].to(device)\n",
    "        lyrics_vectorized.append(model(batch)[1].tolist())\n",
    "        del batch\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vects1 = df[\"vect\"][:5].tolist()\n",
    "vects2 = df_events[\"vect\"][:2].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = cosine_similarity(df[\"vect\"].tolist(), df_events[\"vect\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim2 = euclidean_distances(df[\"vect\"].tolist(), df_events[\"vect\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterRefs(row):\n",
    "    filteredRefs = []\n",
    "    for ref in row[\"Refs\"]:\n",
    "        if row[\"sims\"][ref] > 0.94:\n",
    "            filteredRefs.append(ref)\n",
    "    return filteredRefs\n",
    "\n",
    "df[\"filteredRefs\"] = df.apply(filterRefs, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577\n",
      "10.458577569363033\n"
     ]
    }
   ],
   "source": [
    "print(df[\"filteredRefs\"].apply(len).max())\n",
    "print(df[\"filteredRefs\"].apply(len).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      3173\n",
       "1       338\n",
       "2       252\n",
       "3       130\n",
       "4       119\n",
       "       ... \n",
       "167       1\n",
       "363       1\n",
       "106       1\n",
       "122       1\n",
       "455       1\n",
       "Name: filteredRefs, Length: 201, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"filteredRefs\"].apply(len).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479\n",
      "48.00627802690583\n",
      "1557\n",
      "310.247533632287\n"
     ]
    }
   ],
   "source": [
    "print(df_events[\"filteredRefs\"].apply(len).max())\n",
    "print(df_events[\"filteredRefs\"].apply(len).mean())\n",
    "print(df_events[\"Refs\"].apply(len).max())\n",
    "print(df_events[\"Refs\"].apply(len).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577\n",
      "10.458577569363033\n",
      "1083\n",
      "67.59007424775302\n"
     ]
    }
   ],
   "source": [
    "print(df[\"filteredRefs\"].apply(len).max())\n",
    "print(df[\"filteredRefs\"].apply(len).mean())\n",
    "print(df[\"Refs\"].apply(len).max())\n",
    "print(df[\"Refs\"].apply(len).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_vectors = []\n",
    "for batch_lyrics in lyrics_vectorized:\n",
    "    for lyric in batch_lyrics:\n",
    "        lyrics_vectors.append(lyric)\n",
    "        \n",
    "df[\"vect\"] = lyrics_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events[\"text_vect\"] = df_events.Summary.fillna(df_events.Content)\n",
    "df_events[\"word_count\"] = df_events[\"text_vect\"].apply(lambda x : len(x.split(\" \")))\n",
    "df_events[\"text_vect\"] = np.where(df_events[\"word_count\"] > 320, df_events.Content, df_events[\"text_vect\"])\n",
    "df_events[\"word_count\"] = df_events[\"text_vect\"].apply(lambda x : len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_tokenized = torch.Tensor(tokenizer.batch_encode_plus(df_events.text_vect,\n",
    "                                                           pad_to_max_length=True,\n",
    "                                                           padding_side = \"right\",\n",
    "                                                           add_special_tokens=True)[\"input_ids\"]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"events_vects.pickle\", \"rb\") as f:\n",
    "    tmp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"events_vects.pickle\", \"wb\") as f:\n",
    "    pickle.dump(events_vectorized, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_vectorized = []\n",
    "for i in tqdm_notebook(df_events.text_vect):\n",
    "    token_vec = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)\n",
    "    events_vectorized.append(model(token_vec)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Artist.str.contains(\"dire straits\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_vectorized = []\n",
    "batch_size = 8\n",
    "with torch.no_grad():\n",
    "    for i in tqdm_notebook(range(0, len(df_events), batch_size)):\n",
    "        batch = events_tokenized[i: min(i + batch_size, len(df_events))].to(device)\n",
    "        events_vectorized.append(model(batch))\n",
    "        del batch\n",
    "        \n",
    "events_vectorized = [x[1] for x in events_vectorized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_vectors = []\n",
    "for batch_event in events_vectorized:\n",
    "    for event in batch_event:\n",
    "        events_vectors.append(event.tolist())\n",
    "        \n",
    "df_events[\"vect\"] = events_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events.Refs.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"text_vect\"] = df.Summary.fillna(df_events.Content)\n",
    "# df[\"word_count\"] = df[\"Lyrics\"].apply(lambda x : len(x.split(\" \")))\n",
    "# df[\"text_vect\"] = np.where(df[\"word_count\"] > 400, df_events.Content, df_events[\"text_vect\"])\n",
    "# df[\"word_count\"] = df_events[\"text_vect\"].apply(lambda x : len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [ for sent in df_events[\"text_vect\"]]\n",
    "x = torch.tensor(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.docvecs.vectors_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model.docvecs.vectors_docs).to_csv(\"vec.tsv\", index = False, sep = \"\\t\", header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data.tsv\", sep = \"\\t\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
